{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Webdriver를 이용한 크롤링\n",
    "\n",
    "<p align = \"center\">\n",
    "  <img src = \"https://user-images.githubusercontent.com/76269316/164836248-a14323f3-e78d-4b5c-afbc-f466864623de.png\">\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "  네이버 뉴스 페이지\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align = \"center\">\n",
    "  <img src = \"https://user-images.githubusercontent.com/76269316/164833886-5216bdec-6d3f-4690-8675-fca79a114966.png\">\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "  삼성전자 검색 결과\n",
    "</p>\n",
    "\n",
    "네이버 뉴스 페이지에서 오른쪽 상단에 있는 검색창을 통해 관련 기사를 검색하면 아래 페이지로 이동한다. <br>\n",
    "\n",
    "아래 페이지로 이동시 나오는 기사들은 해당 신문사의 페이지로 이동하기 때문에 기사 내용들이 모두 다른 포맷으로 작성돼 있다.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img width=\"1624\" alt=\"스크린샷 2022-04-23 오전 9 06 46\" src=\"https://user-images.githubusercontent.com/76269316/164836426-7ac782e5-73b6-4bec-99af-2516c989a65f.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<img width=\"1624\" alt=\"스크린샷 2022-04-23 오전 9 08 10\" src=\"https://user-images.githubusercontent.com/76269316/164836482-02bf3bce-9d59-4ae3-b090-0a3a2e0d6b0c.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naver API를 이용한 크롤링\n",
    "\n",
    "<p align = \"center\">\n",
    "  <img src = \"https://user-images.githubusercontent.com/76269316/164836860-acaf96f9-24d1-4293-b798-c49dfd16b45f.png\">\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "  네이버 뉴스 API를 통해 크롤링한 뉴스 링크\n",
    "</p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<p align = \"center\">\n",
    "  <img src = \"https://user-images.githubusercontent.com/76269316/164836977-6d162096-a293-4053-96bc-23b63f7dac7b.png\">\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "  크롤링한 뉴스 기사 (네이버)\n",
    "</p>\n",
    "\n",
    "<p align = \"center\">\n",
    "  <img src = \"https://user-images.githubusercontent.com/76269316/164978236-0f75dd28-c9d0-47ce-9212-7f572f1f1c15.png\">\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "  기사 본문에 있는 네이버\n",
    "</p>\n",
    "\n",
    "<br>\n",
    "\n",
    "API를 통해 검색된 기사들은 네이버에서 제공하는 포맷으로 보여지기 때문에 크롤링하기엔 적합하지만, 내용상 연관성이 떨어지는 기사들이 너무 많다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/usr/lib/chromium-browser/chromedriver')\n",
    "\n",
    "from selenium import webdriver\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')  # 브라우저를 띄우지 않음\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVrQshteDIW6",
    "outputId": "802ee608-f734-475f-ea1b-385cdaec9dd4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver', options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleansing(text):\n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail주소제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URL제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)' # 한글 자음, 모음 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '<[^>]*>'         # HTML 태그 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '[^\\w\\s]' # 특수기호제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    text = text.replace('\\n',\"\")  # 개행 문자 제거\n",
    "    text = text.replace('\\t',\"\")\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "INYnIqU6CmUF"
   },
   "outputs": [],
   "source": [
    "# company_code(종목번호), maxpage(크롤링 할  페이지 수), title(제목이 연관된 기사 - True, 제목 + 내용이 연관된 기사 - False) \n",
    "def news_crawling(company_code, maxpage, title=True):\n",
    "    page = 1\n",
    "\n",
    "    title_list = []\n",
    "    article_list = []\n",
    "    link_list = []\n",
    "    date_list = []\n",
    "    newspaper_list = []\n",
    "\n",
    "    while page <= int(maxpage):\n",
    "        print('page: ', page)\n",
    "        if title:  # 제목\n",
    "            url = 'https://finance.naver.com/item/news_news.nhn?code=' + str(company_code) + '&page=' + str(page)\n",
    "        else:  # 제목 + 내용\n",
    "            url = 'https://finance.naver.com/item/news_news.nhn?code=' + str(company_code) + '&page=' + str(page) + '&sm=entity_id.basic'\n",
    "\n",
    "        html = requests.get(url).text\n",
    "        html = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        # 뉴스 제목\n",
    "        titles = html.select('.title')\n",
    "        for title in titles:\n",
    "            title_list.append(re.sub('\\n', '',  title.get_text()))\n",
    "            \n",
    "        # 기사 내용\n",
    "        links = html.select('.title')\n",
    "        \n",
    "        # 기사 링크 parsing\n",
    "        url_list = []\n",
    "        for link in links:\n",
    "            url_list.append('https://finance.naver.com' + link.find('a')['href'])\n",
    "            link_list.append('https://finance.naver.com' + link.find('a')['href'])\n",
    "\n",
    "        for article_url in url_list:\n",
    "            article_list.append(get_article(article_url))\n",
    "\n",
    "        \n",
    "        # 뉴스 날짜\n",
    "        dates = html.select('.date')\n",
    "        for date in dates:\n",
    "            date_list.append(date.get_text())\n",
    "\n",
    "        # 신문사\n",
    "        newspapers = html.select('.info')\n",
    "        for newspaper in newspapers:\n",
    "            newspaper_list.append(newspaper.get_text())\n",
    "            \n",
    "        print(len(title_list),  len(link_list), len(article_list), len(date_list), len(newspaper_list), '\\n')\n",
    "        page += 1\n",
    "\n",
    "    print(len(title_list),  len(link_list), len(article_list), len(date_list), len(newspaper_list))\n",
    "    news_df = pd.DataFrame({'Title': title_list, 'Article': article_list, 'Date': date_list, 'Newspaper': newspaper_list, 'Link': link_list})\n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lhdBNJA-E9h4"
   },
   "outputs": [],
   "source": [
    "def get_article(url):\n",
    "    driver.get(url)\n",
    "\n",
    "    html = driver.find_element(By.XPATH, '//*[@id=\"news_read\"]')\n",
    "    article = cleansing(html.text)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "nqZqXaD2HSiZ",
    "outputId": "f636add9-cb99-41de-9c58-d0637b185530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page:  1\n",
      "14 14 14 14 14 \n",
      "\n",
      "page:  2\n",
      "26 26 26 26 26 \n",
      "\n",
      "page:  3\n",
      "40 40 40 40 40 \n",
      "\n",
      "page:  4\n",
      "56 56 56 56 56 \n",
      "\n",
      "page:  5\n",
      "66 66 66 66 66 \n",
      "\n",
      "page:  6\n",
      "83 83 83 83 83 \n",
      "\n",
      "page:  7\n",
      "93 93 93 93 93 \n",
      "\n",
      "page:  8\n",
      "107 107 107 107 107 \n",
      "\n",
      "page:  9\n",
      "119 119 119 119 119 \n",
      "\n",
      "page:  10\n",
      "137 137 137 137 137 \n",
      "\n",
      "137 137 137 137 137\n"
     ]
    }
   ],
   "source": [
    "news_df = news_crawling('035420', 10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YNw_1DazimVC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'美 증시 급락 충격에 2620선 깨졌던 코스피 2620선 회복외국인기관 매도에도 개인 매수하며 지수 추가 하락 방어코스피가 미국 증시 급락의 영향으로 27일 2 가까이 하락하며 출발했다 연합뉴스서울경제국내 증시는 간밤 미국 증시 급락이 부담으로 작용하며 2620선에서 등락을 반복 중이다 외국인과 기관의 매도세에도 개인의 적극적인 순매수세가 지수 추가 하락을 방어하고 있는 것으로 풀이된다27일 오후 1시 26분 기준 코스피는 전날 대비 3398포인트127 내린 263370에 거래 중이다 장 초반 가파른 하락세로 한때 2615선까지 내려왔던 코스피는 다시 2620선을 회복한 후 근처에서 등락을 반복하고 있다투자자별로는 외국인과 기관이 각 3816억 원 2451억 원을 매도하는 동안 개인이 6287억 원을 매수하며 지수를 지탱하고 있다같은 시간 코스피 시총 상위 10개 종목 모두 파란 불이 켜졌다 삼성전자005930는 전날 대비 166 내린 6만 5000원에 거래되며 52주 신저가를 유지 중이며 LG에너지솔루션373220은 전날 대비 248 내린 41만 3000원에 거래 중이다 SK하이닉스000660 NAVER035420 카카오035720가 각 225 279 200 빠지며 2대 넘는 하락세를 보이고 있다장 초반 미국 증시 급락 공포에 지수가 주저앉았지만 개인들의 저가매수세가 확대되며 2620선을 회복하고 있는 것으로 풀이된다 증시 급락의 위기와 공포를 매수 확대의 기회로 삼아야 한다는 의견도 제시된다 이경민 대신증권 연구원은 현재 글로벌 금융시장은 긴축 부담경기 둔화 및 침체 우려 등을 모두 선반영하고 있는 것으로 보인다면서 2600선이 깨진다면 적극적으로 매수해야 할 순간으로 판단된다고 했다 이 연구원은 2600선 아래로 지수가 하락할 가능성도 배제할 수는 없다면서도 코스피 밸류에이션 저점인 12개월 선행 주가수익비율PER 10배를 밑돌기 시작한 이 시점은 공포를 매수로 대응해야 할 때로 보인다고 했다같은 시각 코스닥은 전날 대비 1706포인트187 내린 89410에 거래 중이다 코스닥에서도 개인의 저가매수는 이어져 외국인과 기관이 각 561억 원 1452억 원 가량 매도하는 동안 개인 홀로 2030억 원을 사들이고 있다코스닥 시총 상위 10개 종목 중에서는 펄어비스263750의 하락이 돋보인다 같은 시간 펄어비스는 전날 대비 2286 내린 7만 5600원에 거래 중이다 펄어비스는 검은사막 모바일이 중국에서 좋은 성적을 거두지 못하자 하락하고 있다 같은 게임주인 카카오게임즈293490도 452 가량 하락 중이다'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.iloc[0]['Article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Z_2yaxJXqqnl"
   },
   "outputs": [],
   "source": [
    "news_df.to_csv('naver.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
